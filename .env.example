# TinyLLM Docker Environment Configuration
# Copy this file to .env and adjust values as needed

# ============================================================================
# Application Configuration
# ============================================================================

# Log configuration
TINYLLM_LOG_LEVEL=INFO
TINYLLM_LOG_FORMAT=json

# Metrics configuration
METRICS_ENABLED=true
METRICS_PORT=9090

# ============================================================================
# Ollama Configuration
# ============================================================================

# Ollama host (use service name in docker-compose)
OLLAMA_HOST=http://ollama:11434
OLLAMA_TIMEOUT_MS=30000
OLLAMA_MAX_RETRIES=3

# ============================================================================
# Redis Configuration
# ============================================================================

# Redis connection
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
# Set a password for production
REDIS_PASSWORD=

# ============================================================================
# Grafana Configuration
# ============================================================================

# Grafana admin credentials (change in production!)
GRAFANA_USER=admin
GRAFANA_PASSWORD=admin

# ============================================================================
# Resource Limits (Optional)
# ============================================================================

# Uncomment and adjust if needed
# TINYLLM_MAX_CONCURRENT_REQUESTS=10
# TINYLLM_REQUEST_TIMEOUT_MS=60000

# ============================================================================
# GPU Configuration (for Ollama)
# ============================================================================

# Uncomment if you have NVIDIA GPU and want to use it
# NVIDIA_VISIBLE_DEVICES=all
# NVIDIA_DRIVER_CAPABILITIES=compute,utility
