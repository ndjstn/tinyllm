{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TinyLLM Demo\n",
        "\n",
        "> **What if each neuron in a neural network was already intelligent?**\n",
        "\n",
        "This notebook demonstrates TinyLLM - a system that treats small LLMs as intelligent neurons in a larger cognitive architecture.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU runtime (T4 or better)\n",
        "- ~6GB VRAM for qwen2.5:3b\n",
        "\n",
        "**Note:** This notebook runs entirely in Colab - no local setup required!"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-ollama"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama server in background\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "time.sleep(5)\n",
        "print(\"Ollama server started!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the models we need\n",
        "!ollama pull qwen2.5:0.5b  # Router (tiny, fast)\n",
        "!ollama pull qwen2.5:3b     # Specialist"
      ],
      "metadata": {
        "id": "pull-models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone and install TinyLLM\n",
        "!git clone https://github.com/ndjstn/tinyllm.git\n",
        "%cd tinyllm\n",
        "!pip install -e . -q"
      ],
      "metadata": {
        "id": "install-tinyllm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Verify Installation"
      ],
      "metadata": {
        "id": "verify-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Ollama is running\n",
        "!curl -s http://localhost:11434/api/tags | python -c \"import sys,json; models=json.load(sys.stdin)['models']; print('Available models:', [m['name'] for m in models])\""
      ],
      "metadata": {
        "id": "check-ollama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test basic import\n",
        "from tinyllm.core.builder import load_graph\n",
        "from tinyllm.core.executor import Executor\n",
        "from tinyllm.core.message import TaskPayload\n",
        "\n",
        "print(\"TinyLLM imported successfully!\")"
      ],
      "metadata": {
        "id": "test-import"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Run Queries"
      ],
      "metadata": {
        "id": "queries-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from pathlib import Path\n",
        "\n",
        "# Load the graph\n",
        "graph = load_graph(Path(\"graphs/multi_domain.yaml\"))\n",
        "executor = Executor(graph)\n",
        "\n",
        "async def query(text):\n",
        "    \"\"\"Run a query through TinyLLM.\"\"\"\n",
        "    task = TaskPayload(content=text)\n",
        "    response = await executor.execute(task)\n",
        "    return response\n",
        "\n",
        "# Test query\n",
        "response = await query(\"What is 15 + 27?\")\n",
        "print(f\"Success: {response.success}\")\n",
        "print(f\"Response: {response.content[:500]}...\" if len(response.content) > 500 else f\"Response: {response.content}\")"
      ],
      "metadata": {
        "id": "run-query"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different query types\n",
        "queries = [\n",
        "    \"Write a Python function to check if a number is prime\",\n",
        "    \"What causes earthquakes?\",\n",
        "    \"Calculate 15% of 240, then add 50\",\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Query: {q}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    response = await query(q)\n",
        "    print(f\"Response ({len(response.content)} chars):\")\n",
        "    print(response.content[:800])"
      ],
      "metadata": {
        "id": "multiple-queries"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Benchmarks"
      ],
      "metadata": {
        "id": "benchmarks-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the benchmark suite\n",
        "!python benchmarks/run_benchmarks.py"
      ],
      "metadata": {
        "id": "run-benchmarks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run adversarial tests to find weaknesses\n",
        "!python benchmarks/adversarial_test.py"
      ],
      "metadata": {
        "id": "run-adversarial"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Visualize Results"
      ],
      "metadata": {
        "id": "visualize-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate visualizations\n",
        "!python benchmarks/create_visuals.py\n",
        "\n",
        "# Display the dashboard\n",
        "from IPython.display import Image\n",
        "Image(filename='benchmarks/results/performance_dashboard.png', width=800)"
      ],
      "metadata": {
        "id": "show-dashboard"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show adversarial test results\n",
        "import json\n",
        "\n",
        "with open('benchmarks/results/adversarial_test.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"Adversarial Test Summary\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Overall Pass Rate: {data['summary']['pass_rate']:.1f}%\")\n",
        "print(\"\\nBy Category:\")\n",
        "for cat, stats in data['summary']['by_category'].items():\n",
        "    print(f\"  {cat}: {stats['passed']}/{stats['total']} ({stats['rate']:.0f}%)\")"
      ],
      "metadata": {
        "id": "show-adversarial"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Interactive Chat"
      ],
      "metadata": {
        "id": "chat-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple interactive chat\n",
        "def chat():\n",
        "    \"\"\"Interactive chat with TinyLLM.\"\"\"\n",
        "    print(\"TinyLLM Chat (type 'quit' to exit)\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"You: \")\n",
        "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "\n",
        "            response = asyncio.get_event_loop().run_until_complete(query(user_input))\n",
        "            print(f\"TinyLLM: {response.content}\\n\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "\n",
        "# Uncomment to start chat:\n",
        "# chat()"
      ],
      "metadata": {
        "id": "interactive-chat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources\n",
        "\n",
        "- **GitHub**: https://github.com/ndjstn/tinyllm\n",
        "- **500 Task Roadmap**: [docs/TASK_ROADMAP.md](https://github.com/ndjstn/tinyllm/blob/master/docs/TASK_ROADMAP.md)\n",
        "- **Benchmarks**: [benchmarks/README.md](https://github.com/ndjstn/tinyllm/blob/master/benchmarks/README.md)\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "| Test | Pass Rate | Notes |\n",
        "|------|-----------|-------|\n",
        "| Standard | 100% | Basic queries work |\n",
        "| Stress | 100% | Scales to extreme difficulty |\n",
        "| Adversarial | ~52% | False premises, hallucinations |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Add chain-of-thought reasoning to improve adversarial performance\n",
        "2. Implement self-morphing architecture\n",
        "3. Add solution memory for learning"
      ],
      "metadata": {
        "id": "resources"
      }
    }
  ]
}
